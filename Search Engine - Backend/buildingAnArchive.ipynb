{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtml(doc_num, mid_string):\n",
    "  doc_id = str(0) * (5 - len(str(doc_num))) + str(doc_num)\n",
    "  url = \"http://arxiv.org/abs/\"+ mid_string+\".\"+ doc_id\n",
    "  response=requests.get(url)\n",
    "  html = response.text\n",
    "  return html\n",
    "\n",
    "def getTitle(html):\n",
    "  return html[html.index('<title>') + 20:html.index('</title>')]\n",
    "\n",
    "def getAbstract(text):\n",
    "    start_ind = text.index('\"citation_abstract\" content=') + 29\n",
    "    from_abs = text[start_ind:]\n",
    "    next_new_line = [m.start() for m in re.finditer(r'\" />',from_abs)][0] + start_ind\n",
    "    abstract = text[start_ind:next_new_line]\n",
    "    abstact = abstract.replace('-','')\n",
    "    abstact = abstract.replace('\\n',' ')\n",
    "    return abstact\n",
    "\n",
    "def getAuthors(html):\n",
    "  auts = html[html.index('citation_author')+ 16:html.index('\"citation_date\" content=\"')]\n",
    "  auts = auts.replace('\"citation_author\"', '')\n",
    "  authors = []\n",
    "  quote_positions = [m.start() for m in re.finditer(r'\"', auts)]\n",
    "  for i in range(0,len(quote_positions), 2):\n",
    "    name = auts[quote_positions[i]+1: quote_positions[i+1]].replace(' ', '')\n",
    "    authors.append(name.split(','))\n",
    "  return authors\n",
    "\n",
    "def getDate(text):\n",
    "    start_ind = text.index('\"citation_date\" content=\"') + 25\n",
    "    from_abs = text[start_ind:]\n",
    "    next_new_line = [m.start() for m in re.finditer(r'\" />',from_abs)][0] + start_ind\n",
    "    date = text[start_ind:next_new_line]\n",
    "    year = int(date[:4])\n",
    "    month = int(date[5:7])\n",
    "    return [date, year, month]\n",
    "\n",
    "def getYear(arr):\n",
    "  return arr[1]\n",
    "\n",
    "def getMonth(arr):\n",
    "  return arr[2]\n",
    "\n",
    "def getLink(doc_num):\n",
    "  doc_id = str(0) * (5 - len(str(doc_num))) + str(doc_num)\n",
    "  url = \"http://arxiv.org/pdf/2301.\"+ doc_id + \".pdf\"\n",
    "  return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArchive(archive, num_of_docs, starting_num, mid_string):\n",
    "  doc_num = starting_num\n",
    "  lst = []\n",
    "  while(doc_num<=num_of_docs):\n",
    "      try:\n",
    "        html = getHtml(doc_num, mid_string)\n",
    "      except:\n",
    "        return archive\n",
    "      row = {}\n",
    "      row['DocId'] = doc_num\n",
    "      try:\n",
    "        row['Title']= getTitle(html)\n",
    "      except:\n",
    "        row['Title'] = ''\n",
    "      try:\n",
    "        row['Date']= getDate(html)\n",
    "        row['Month'] = row['Date'][2]\n",
    "        row['Year'] = row['Date'][1]\n",
    "\n",
    "      except:\n",
    "        row['Date'] = ['',0,0]\n",
    "      try:\n",
    "        row['Authors']= getAuthors(html)\n",
    "      except:\n",
    "        row['Authors'] = []\n",
    "      try:\n",
    "        row['Abstract'] = getAbstract(html)\n",
    "      except:\n",
    "        row['Abstract'] = ''\n",
    "      doc_id = str(0) * (5 - len(str(doc_num))) + str(doc_num)\n",
    "      row['Link'] = \"http://arxiv.org/pdf/\"+ mid_string + \".\" + doc_id + \".pdf\"\n",
    "      row['Cornell_Index'] = mid_string\n",
    "      doc_num = doc_num + 1\n",
    "      lst.append(row)\n",
    "      #archive.loc[len(archive)] = row\n",
    "      #archive = archive.append(row, ignore_index = True)\n",
    "  archive = pd.DataFrame(lst)  \n",
    "  return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = pd.DataFrame()\n",
    "#buildArchive(archive, num_of_docs, starting_num, mid_string)\n",
    "archive = buildArchive(archive, 5000, 1, '2304')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n",
    "from utils.setupFunctions import getStopwords\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_lm_model = BertModel.from_pretrained(\"Models/\")\n",
    "mini_lm_tokenizer = BertTokenizerFast.from_pretrained(\"Models/Tokenizer\")\n",
    "stopwords = getStopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  return_str = []\n",
    "  for word in text.split(' '):\n",
    "    if (word.isalnum()):\n",
    "      if(word.lower() not in stopwords):\n",
    "        return_str.append(word)\n",
    "  return return_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['Title_processed'] = archive['Title'].apply(preprocess)\n",
    "archive['Abstract_processed'] = archive['Abstract'].apply(preprocess)\n",
    "archive['TandA'] = archive['Title_processed'] + ' ' + archive['Abstract_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def getEmbeddings(text, settings):    \n",
    "    #Tokenize sentences\n",
    "    #mini_lm_model = BertModel.from_pretrained(\"Models/\")\n",
    "    #mini_lm_tokenizer = BertTokenizerFast.from_pretrained(\"Models/Tokenizer\")\n",
    "\n",
    "    encoded_input = mini_lm_tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt',is_split_into_words=True)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "      model_output = mini_lm_model(**encoded_input)\n",
    "\n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['Embeddings'] = torch.dtype\n",
    "#corpus['Month'] = corpus['Date'].apply(getMonth)\n",
    "for i in range(0, len(archive)):\n",
    "  archive.at[i, 'Embeddings'] = getEmbeddings(archive.iloc[i]['raw_keywords'],mini_lm_model, mini_lm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
